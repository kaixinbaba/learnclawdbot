---
title: OpenClaw + Ollama：免费在本地运行你的 AI 助手
description: 详细教程：如何用 Ollama 本地模型驱动 OpenClaw，零 API 费用、完全离线、数据不出本机。支持 Llama、Qwen、DeepSeek 等开源模型。
slug: /openclaw-ollama
tags: ollama, local-ai, models, guide
publishedAt: 2026-02-03
status: published
visibility: public
featuredImageUrl: /images/features/guide_head_new.jpg
---

用 Ollama 跑本地模型，零费用、零延迟、数据不出你的电脑。

## 为什么用 Ollama？

- **免费** — 所有模型成本 $0，跑多少次都不心疼
- **隐私** — 数据完全本地处理，不经过任何第三方服务器
- **离线可用** — 没网也能用，飞机上、地铁里照样工作
- **简单** — 一条命令下载模型，OpenClaw 自动发现并配置

如果你在意 API 账单或数据隐私，Ollama 是最佳选择。

## 前置条件

开始之前，确保你已经：

- 安装了 OpenClaw（[查看安装教程](/blog/install-openclaw)）
- 内存至少 16GB（推荐 32GB，运行大模型更流畅）
- GPU 可选但强烈推荐（没有也能跑，只是慢一些）

## 第一步：安装 Ollama

在 macOS 或 Linux 上：

```bash
curl -fsSL https://ollama.ai/install.sh | sh
```

Windows 用户直接去 [ollama.ai](https://ollama.ai) 下载安装包。

安装完成后，Ollama 会自动在后台运行，监听 `http://localhost:11434`。

## 第二步：拉取模型

Ollama 的模型库很丰富，挑一个适合你的：

```bash
# 日常对话 - 轻量快速
ollama pull llama3.3

# 编程助手 - 代码生成和理解
ollama pull qwen2.5-coder:32b

# 深度推理 - 复杂问题分析
ollama pull deepseek-r1:32b

# 轻量级 - 资源受限环境
ollama pull mistral
```

### 推荐模型对照表

| 模型 | 适合场景 | 内存需求 |
|------|---------|---------|
| llama3.3 | 日常对话、通用任务 | 8GB+ |
| qwen2.5-coder:32b | 编程辅助、代码审查 | 32GB+ |
| deepseek-r1:32b | 深度推理、复杂分析 | 32GB+ |
| mistral | 轻量快速、资源受限 | 8GB+ |

首次拉取会下载几个 GB 的模型文件，耐心等待。后续使用都是本地加载，秒开。

## 第三步：启用 Ollama（最简方式）

OpenClaw 内置了 Ollama 自动发现机制。只需设置一个环境变量：

```bash
export OLLAMA_API_KEY="ollama-local"
```

把这行加到你的 `~/.zshrc` 或 `~/.bashrc`，让它永久生效：

```bash
echo 'export OLLAMA_API_KEY="ollama-local"' >> ~/.zshrc
source ~/.zshrc
```

**就这样。** OpenClaw 会自动：
- 发现本地运行的 Ollama 服务
- 扫描所有已下载的模型
- 筛选出支持工具调用的模型
- 将它们注册为可用模型

不需要手写配置文件，不需要指定端口。一切自动完成。

## 第四步：设为默认模型

让 OpenClaw 默认使用 Ollama 模型：

**方法一：命令行配置**

```bash
openclaw config set agents.defaults.model.primary "ollama/llama3.3"
```

**方法二：编辑配置文件**

打开 OpenClaw 配置文件（通常在 `~/.openclaw/config.json5`），添加：

```json5
{
  agents: {
    defaults: {
      model: {
        primary: "ollama/llama3.3"
      }
    }
  }
}
```

重启 OpenClaw 后，所有对话默认走 Ollama。

## 进阶配置

### 远程 Ollama 服务器

如果你把 Ollama 跑在另一台机器上（比如家里的 GPU 服务器），可以手动指定 URL：

```json5
{
  models: {
    providers: {
      ollama: {
        baseUrl: "http://192.168.1.100:11434/v1",
        apiKey: "ollama-local",
        api: "openai-completions"
      }
    }
  }
}
```

### 手动定义模型

某些情况下你可能需要显式配置模型：

- Ollama 在远程主机上，自动发现失效
- 想强制指定上下文窗口大小
- 使用自定义模型或实验性模型

手动配置示例：

```json5
{
  models: {
    providers: {
      ollama: {
        baseUrl: "http://localhost:11434/v1",
        apiKey: "ollama-local",
        api: "openai-completions",
        models: {
          "llama3.3": {
            id: "llama3.3",
            capabilities: ["tools", "thinking"],
            contextWindow: 128000,
            maxOutputTokens: 4096,
            costs: { prompt: 0, completion: 0 }
          }
        }
      }
    }
  }
}
```

**注意：** 显式定义 `providers.ollama` 会禁用自动发现。确保你知道自己在做什么。

## 自动发现机制

OpenClaw 的 Ollama 集成很智能。它会：

1. **查询模型列表** — 调用 `/api/tags` 获取所有已下载的模型
2. **检查能力** — 对每个模型调用 `/api/show`，读取 `modelinfo` 元数据
3. **筛选工具调用** — 只保留支持 `tools` 的模型（不支持的会被过滤）
4. **检测推理能力** — 自动识别是否支持 `thinking`（如 DeepSeek-R1）
5. **读取上下文窗口** — 从 `context_length` 字段获取准确值
6. **零成本标记** — 所有 Ollama 模型成本自动设为 $0

这意味着你只需 `ollama pull` 一个新模型，OpenClaw 立刻就能用，无需任何配置。

## 与云端模型对比

| 维度 | Ollama 本地模型 | Claude/GPT 云端模型 |
|------|----------------|-------------------|
| **成本** | 完全免费 | 按 token 计费，大量使用很贵 |
| **速度** | 取决于硬件（GPU 快，CPU 慢） | 稳定快速，延迟低 |
| **能力** | 开源模型有差距，但快速追赶中 | 顶尖水平，复杂任务更可靠 |
| **隐私** | 数据不出本机 | 数据传到云端 |
| **离线** | 完全离线可用 | 必须联网 |

**推荐用法：**
- 日常对话、代码补全 → Ollama（省钱）
- 复杂推理、长文档分析 → 云端模型（质量）
- 敏感数据处理 → Ollama（隐私）

你可以在 OpenClaw 中同时配置多个模型提供商，按需切换。

## 常见问题

### Ollama 模型没被发现

**原因：** 环境变量未设置，或显式配置了 `providers.ollama` 导致自动发现被禁用。

**解决：**
```bash
# 检查环境变量
echo $OLLAMA_API_KEY

# 应该输出：ollama-local
# 如果没有，设置它：
export OLLAMA_API_KEY="ollama-local"
```

如果配置文件中有 `models.providers.ollama.models = \{ ... \}`，删掉它让自动发现生效。

### 没有可用模型

**原因：** 已下载的模型不支持工具调用（function calling）。

OpenClaw 只会发现支持 `tools` 的模型。如果模型太老或能力受限，会被自动过滤。

**解决：**
- 使用较新的模型（如 llama3.3、qwen2.5、mistral）
- 或手动配置模型（参考上面的"手动定义模型"）

### 连接被拒

**原因：** Ollama 服务未运行。

**解决：**
```bash
# 测试 Ollama 是否运行
curl http://localhost:11434/api/tags

# 应该返回模型列表 JSON
# 如果连接失败，启动 Ollama：
ollama serve
```

macOS/Windows 安装包会自动启动服务，Linux 手动安装需要自己运行 `ollama serve`。

## 下一步

- [OpenClaw 安装指南](/blog/install-openclaw) — 从零开始安装 OpenClaw
- [新手入门教程](/blog/guide) — 学习 OpenClaw 的核心功能
- [集成 OpenAI](/blog/integrations/openai) — 配置 GPT 系列云端模型
- [集成 Claude](/blog/integrations/anthropic) — 配置 Anthropic Claude 模型

---

**总结：** Ollama + OpenClaw 让你以零成本运行私有 AI 助手。配置简单到只需一个环境变量，模型自动发现，数据完全本地。对于开发者来说，这是探索 AI 能力、保护隐私、控制成本的最佳方案。

试试吧，你会爱上这种感觉。
