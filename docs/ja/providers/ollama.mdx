---
summary: "Ollama（ローカル LLM ランタイム）で OpenClaw を実行する"
read_when:
  - Ollama 経由でローカルモデルで OpenClaw を実行したい
  - Ollama のセットアップと設定ガイダンスが必要
---
# Ollama

Ollama は、マシン上でオープンソースモデルを簡単に実行できるローカル LLM ランタイムです。OpenClaw は Ollama の OpenAI 互換 API と統合され、`OLLAMA_API_KEY`（または認証プロファイル）でオプトインし、明示的な `models.providers.ollama` エントリを定義しない場合、**ツール対応モデルを自動検出**できます。

## クイックスタート

1) Ollama をインストール: https://ollama.ai

2) モデルをプル：

```bash
ollama pull llama3.3
# または
ollama pull qwen2.5-coder:32b
# または
ollama pull deepseek-r1:32b
```

3) OpenClaw 用に Ollama を有効化（任意の値で動作します。Ollama は実際のキーを必要としません）：

```bash
# 環境変数を設定
export OLLAMA_API_KEY="ollama-local"

# または設定ファイルで設定
openclaw config set models.providers.ollama.apiKey "ollama-local"
```

4) Ollama モデルを使用：

```json5
{
  agents: {
    defaults: {
      model: { primary: "ollama/llama3.3" }
    }
  }
}
```

## モデル検出（暗黙的プロバイダ）

`OLLAMA_API_KEY`（または認証プロファイル）を設定し、`models.providers.ollama` を定義**しない**場合、OpenClaw は `http://127.0.0.1:11434` のローカル Ollama インスタンスからモデルを検出します：

- `/api/tags` と `/api/show` をクエリ
- `tools` 機能を報告するモデルのみを保持
- モデルが `thinking` を報告する場合は `reasoning` をマーク
- 利用可能な場合は `model_info["<arch>.context_length"]` から `contextWindow` を読み取り
- `maxTokens` をコンテキストウィンドウの 10 倍に設定
- すべてのコストを `0` に設定

これにより、Ollama の機能とカタログを合わせたまま、手動モデルエントリを回避できます。

利用可能なモデルを確認するには：

```bash
ollama list
openclaw models list
```

新しいモデルを追加するには、Ollama でプルするだけです：

```bash
ollama pull mistral
```

新しいモデルは自動的に検出され、使用可能になります。

`models.providers.ollama` を明示的に設定すると、自動検出はスキップされ、モデルを手動で定義する必要があります（以下を参照）。

## 設定

### 基本セットアップ（暗黙的検出）

Ollama を有効にする最もシンプルな方法は環境変数経由です：

```bash
export OLLAMA_API_KEY="ollama-local"
```

### 明示的セットアップ（手動モデル）

明示的設定を使用する場合：
- Ollama が別のホスト/ポートで実行されている。
- 特定のコンテキストウィンドウまたはモデルリストを強制したい。
- ツールサポートを報告しないモデルを含めたい。

```json5
{
  models: {
    providers: {
      ollama: {
        // OpenAI 互換 API 用の /v1 を含むホストを使用
        baseUrl: "http://ollama-host:11434/v1",
        apiKey: "ollama-local",
        api: "openai-completions",
        models: [
          {
            id: "llama3.3",
            name: "Llama 3.3",
            reasoning: false,
            input: ["text"],
            cost: { input: 0, output: 0, cacheRead: 0, cacheWrite: 0 },
            contextWindow: 8192,
            maxTokens: 8192 * 10
          }
        ]
      }
    }
  }
}
```

`OLLAMA_API_KEY` が設定されている場合、プロバイダエントリで `apiKey` を省略でき、OpenClaw が可用性チェックのために入力します。

### カスタムベース URL（明示的設定）

Ollama が異なるホストまたはポートで実行されている場合（明示的設定は自動検出を無効にするため、モデルを手動で定義します）：

```json5
{
  models: {
    providers: {
      ollama: {
        apiKey: "ollama-local",
        baseUrl: "http://ollama-host:11434/v1"
      }
    }
  }
}
```

### モデル選択

設定が完了すると、すべての Ollama モデルが利用可能になります：

```json5
{
  agents: {
    defaults: {
      model: {
        primary: "ollama/llama3.3",
        fallback: ["ollama/qwen2.5-coder:32b"]
      }
    }
  }
}
```

## 高度な設定

### 推論モデル

OpenClaw は、Ollama が `/api/show` で `thinking` を報告した場合、モデルを推論対応としてマークします：

```bash
ollama pull deepseek-r1:32b
```

### モデルコスト

Ollama は無料でローカルで実行されるため、すべてのモデルコストは $0 に設定されています。

### コンテキストウィンドウ

自動検出されたモデルの場合、OpenClaw は利用可能な場合は Ollama が報告するコンテキストウィンドウを使用し、そうでない場合はデフォルトの `8192` を使用します。明示的なプロバイダ設定で `contextWindow` と `maxTokens` を上書きできます。

## トラブルシューティング

### Ollama が検出されない

Ollama が実行されており、`OLLAMA_API_KEY`（または認証プロファイル）を設定し、明示的な `models.providers.ollama` エントリを定義**していない**ことを確認してください：

```bash
ollama serve
```

そして API がアクセス可能であることを確認します：

```bash
curl http://localhost:11434/api/tags
```

### 利用可能なモデルがない

OpenClaw はツールサポートを報告するモデルのみを自動検出します。モデルがリストされていない場合は、次のいずれかを実行します：
- ツール対応モデルをプルする、または
- `models.providers.ollama` でモデルを明示的に定義する。

モデルを追加するには：

```bash
ollama list  # インストール済みを確認
ollama pull llama3.3  # モデルをプル
```

### 接続が拒否される

Ollama が正しいポートで実行されていることを確認してください：

```bash
# Ollama が実行されているか確認
ps aux | grep ollama

# または Ollama を再起動
ollama serve
```

## 関連項目

- [モデルプロバイダ](/concepts/model-providers) - すべてのプロバイダの概要
- [モデル選択](/concepts/models) - モデルの選択方法
- [設定](/gateway/configuration) - 完全な設定リファレンス
