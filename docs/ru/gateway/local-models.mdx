---
summary: "Запуск OpenClaw на локальных LLM (LM Studio, vLLM, LiteLLM, пользовательские конечные точки OpenAI)"
read_when:
  - Вы хотите обслуживать модели со своего GPU-бокса
  - Вы подключаете LM Studio или OpenAI-совместимый прокси
  - Вам нужно самое безопасное руководство по локальным моделям
---
# Локальные модели

Локальные возможны, но OpenClaw ожидает большой контекст + сильную защиту от инъекции промптов. Маленькие карты обрезают контекст и пропускают безопасность. Стремитесь высоко: **≥2 максимально укомплектованных Mac Studios или эквивалентная GPU-установка (~$30k+)**. Одна **24 ГБ** GPU работает только для более легких промптов с более высокой задержкой. Используйте **самый большой / полноразмерный вариант модели, который вы можете запустить**; агрессивно квантованные или "малые" контрольные точки повышают риск инъекции промптов (см. [Безопасность](/gateway/security)).

## Рекомендуется: LM Studio + MiniMax M2.1 (Responses API, полный размер)

Лучший текущий локальный стек. Загрузите MiniMax M2.1 в LM Studio, включите локальный сервер (по умолчанию `http://127.0.0.1:1234`), и используйте Responses API, чтобы держать рассуждения отдельно от финального текста.

```json5
\{
  agents: \{
    defaults: \{
      model: \{ primary: "lmstudio/minimax-m2.1-gs32" \},
      models: \{
        "anthropic/claude-opus-4-5": \{ alias: "Opus" \},
        "lmstudio/minimax-m2.1-gs32": \{ alias: "Minimax" \}
      \}
    \}
  \},
  models: \{
    mode: "merge",
    providers: \{
      lmstudio: \{
        baseUrl: "http://127.0.0.1:1234/v1",
        apiKey: "lmstudio",
        api: "openai-responses",
        models: [
          \{
            id: "minimax-m2.1-gs32",
            name: "MiniMax M2.1 GS32",
            reasoning: false,
            input: ["text"],
            cost: \{ input: 0, output: 0, cacheRead: 0, cacheWrite: 0 \},
            contextWindow: 196608,
            maxTokens: 8192
          \}
        ]
      \}
    \}
  \}
\}
```

**Чек-лист настройки**
- Установите LM Studio: https://lmstudio.ai
- В LM Studio загрузите **самую большую доступную сборку MiniMax M2.1** (избегайте "малых"/сильно квантованных вариантов), запустите сервер, подтвердите, что `http://127.0.0.1:1234/v1/models` ее перечисляет.
- Держите модель загруженной; холодная загрузка добавляет задержку запуска.
- Отрегулируйте `contextWindow`/`maxTokens`, если ваша сборка LM Studio отличается.
- Для WhatsApp придерживайтесь Responses API, чтобы отправлялся только финальный текст.

Держите настроенными размещенные модели, даже при работе локально; используйте `models.mode: "merge"`, чтобы резервы оставались доступными.

### Гибридная конфигурация: размещенный первичный, локальный резерв

```json5
\{
  agents: \{
    defaults: \{
      model: \{
        primary: "anthropic/claude-sonnet-4-5",
        fallbacks: ["lmstudio/minimax-m2.1-gs32", "anthropic/claude-opus-4-5"]
      \},
      models: \{
        "anthropic/claude-sonnet-4-5": \{ alias: "Sonnet" \},
        "lmstudio/minimax-m2.1-gs32": \{ alias: "MiniMax Local" \},
        "anthropic/claude-opus-4-5": \{ alias: "Opus" \}
      \}
    \}
  \},
  models: \{
    mode: "merge",
    providers: \{
      lmstudio: \{
        baseUrl: "http://127.0.0.1:1234/v1",
        apiKey: "lmstudio",
        api: "openai-responses",
        models: [
          \{
            id: "minimax-m2.1-gs32",
            name: "MiniMax M2.1 GS32",
            reasoning: false,
            input: ["text"],
            cost: \{ input: 0, output: 0, cacheRead: 0, cacheWrite: 0 \},
            contextWindow: 196608,
            maxTokens: 8192
          \}
        ]
      \}
    \}
  \}
\}
```

### Локально-первый с размещенной страховочной сетью

Поменяйте местами порядок первичного и резервного; держите тот же блок провайдеров и `models.mode: "merge"`, чтобы вы могли откатиться к Sonnet или Opus, когда локальный бокс не работает.

### Региональный хостинг / маршрутизация данных

- Размещенные варианты MiniMax/Kimi/GLM также существуют на OpenRouter с конечными точками, привязанными к региону (например, размещенными в США). Выберите региональный вариант там, чтобы держать трафик в выбранной юрисдикции, все еще используя `models.mode: "merge"` для резервов Anthropic/OpenAI.
- Только локальные остаются самым сильным путем конфиденциальности; размещенная региональная маршрутизация — это средний путь, когда вам нужны функции провайдера, но вы хотите контроль над потоком данных.

## Другие OpenAI-совместимые локальные прокси

vLLM, LiteLLM, OAI-proxy или пользовательские шлюзы работают, если они предоставляют конечную точку `/v1` в стиле OpenAI. Замените блок провайдера выше на вашу конечную точку и идентификатор модели:

```json5
\{
  models: \{
    mode: "merge",
    providers: \{
      local: \{
        baseUrl: "http://127.0.0.1:8000/v1",
        apiKey: "sk-local",
        api: "openai-responses",
        models: [
          \{
            id: "my-local-model",
            name: "Local Model",
            reasoning: false,
            input: ["text"],
            cost: \{ input: 0, output: 0, cacheRead: 0, cacheWrite: 0 \},
            contextWindow: 120000,
            maxTokens: 8192
          \}
        ]
      \}
    \}
  \}
\}
```

Держите `models.mode: "merge"`, чтобы размещенные модели оставались доступными в качестве резервов.

## Устранение неполадок
- Шлюз может достичь прокси? `curl http://127.0.0.1:1234/v1/models`.
- Модель LM Studio выгружена? Перезагрузите; холодный старт — распространенная причина "зависания".
- Ошибки контекста? Понизьте `contextWindow` или поднимите лимит вашего сервера.
- Безопасность: локальные модели пропускают фильтры на стороне провайдера; держите агентов узкими и компактность включенной, чтобы ограничить радиус взрыва инъекции промптов.
