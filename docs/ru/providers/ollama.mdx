---
summary: "Запуск OpenClaw с Ollama (локальная среда выполнения LLM)"
read_when:
  - Вы хотите запустить OpenClaw с локальными моделями через Ollama
  - Вам нужно руководство по настройке и конфигурации Ollama
---
# Ollama

Ollama — это локальная среда выполнения LLM, которая упрощает запуск моделей с открытым исходным кодом на вашей машине. OpenClaw интегрируется с OpenAI-совместимым API Ollama и может **автоматически обнаруживать модели с поддержкой инструментов**, когда вы соглашаетесь с `OLLAMA_API_KEY` (или профилем аутентификации) и не определяете явную запись `models.providers.ollama`.

## Быстрый старт

1) Установите Ollama: https://ollama.ai

2) Загрузите модель:

```bash
ollama pull llama3.3
# или
ollama pull qwen2.5-coder:32b
# или
ollama pull deepseek-r1:32b
```

3) Включите Ollama для OpenClaw (подойдет любое значение; Ollama не требует реального ключа):

```bash
# Установите переменную окружения
export OLLAMA_API_KEY="ollama-local"

# Или настройте в файле конфигурации
openclaw config set models.providers.ollama.apiKey "ollama-local"
```

4) Используйте модели Ollama:

```json5
{
  agents: {
    defaults: {
      model: { primary: "ollama/llama3.3" }
    }
  }
}
```

## Обнаружение моделей (неявный провайдер)

Когда вы устанавливаете `OLLAMA_API_KEY` (или профиль аутентификации) и **не** определяете `models.providers.ollama`, OpenClaw обнаруживает модели из локального экземпляра Ollama на `http://127.0.0.1:11434`:

- Запрашивает `/api/tags` и `/api/show`
- Сохраняет только модели, которые сообщают о возможности `tools`
- Отмечает `reasoning`, когда модель сообщает `thinking`
- Читает `contextWindow` из `model_info["<arch>.context_length"]`, когда доступно
- Устанавливает `maxTokens` в 10× от окна контекста
- Устанавливает все затраты в `0`

Это избавляет от необходимости вручную вводить модели, сохраняя каталог в соответствии с возможностями Ollama.

Чтобы увидеть, какие модели доступны:

```bash
ollama list
openclaw models list
```

Чтобы добавить новую модель, просто загрузите ее с помощью Ollama:

```bash
ollama pull mistral
```

Новая модель будет автоматически обнаружена и доступна для использования.

Если вы явно установили `models.providers.ollama`, автоматическое обнаружение пропускается, и вы должны определить модели вручную (см. ниже).

## Конфигурация

### Базовая настройка (неявное обнаружение)

Самый простой способ включить Ollama — через переменную окружения:

```bash
export OLLAMA_API_KEY="ollama-local"
```

### Явная настройка (ручные модели)

Используйте явную конфигурацию, когда:
- Ollama работает на другом хосте/порту.
- Вы хотите принудительно установить определенные окна контекста или списки моделей.
- Вы хотите включить модели, которые не сообщают о поддержке инструментов.

```json5
{
  models: {
    providers: {
      ollama: {
        // Используйте хост, который включает /v1 для API, совместимых с OpenAI
        baseUrl: "http://ollama-host:11434/v1",
        apiKey: "ollama-local",
        api: "openai-completions",
        models: [
          {
            id: "llama3.3",
            name: "Llama 3.3",
            reasoning: false,
            input: ["text"],
            cost: { input: 0, output: 0, cacheRead: 0, cacheWrite: 0 },
            contextWindow: 8192,
            maxTokens: 8192 * 10
          }
        ]
      }
    }
  }
}
```

Если установлен `OLLAMA_API_KEY`, вы можете опустить `apiKey` в записи провайдера, и OpenClaw заполнит его для проверок доступности.

### Пользовательский базовый URL (явная конфигурация)

Если Ollama работает на другом хосте или порту (явная конфигурация отключает автоматическое обнаружение, поэтому определите модели вручную):

```json5
{
  models: {
    providers: {
      ollama: {
        apiKey: "ollama-local",
        baseUrl: "http://ollama-host:11434/v1"
      }
    }
  }
}
```

### Выбор модели

После настройки все ваши модели Ollama доступны:

```json5
{
  agents: {
    defaults: {
      model: {
        primary: "ollama/llama3.3",
        fallback: ["ollama/qwen2.5-coder:32b"]
      }
    }
  }
}
```

## Расширенное

### Модели рассуждения

OpenClaw отмечает модели как способные к рассуждению, когда Ollama сообщает `thinking` в `/api/show`:

```bash
ollama pull deepseek-r1:32b
```

### Стоимость моделей

Ollama бесплатен и работает локально, поэтому все затраты на модели установлены в $0.

### Окна контекста

Для автоматически обнаруженных моделей OpenClaw использует окно контекста, сообщаемое Ollama, когда доступно, в противном случае по умолчанию используется `8192`. Вы можете переопределить `contextWindow` и `maxTokens` в явной конфигурации провайдера.

## Устранение неполадок

### Ollama не обнаружен

Убедитесь, что Ollama работает и что вы установили `OLLAMA_API_KEY` (или профиль аутентификации), и что вы **не** определили явную запись `models.providers.ollama`:

```bash
ollama serve
```

И что API доступен:

```bash
curl http://localhost:11434/api/tags
```

### Нет доступных моделей

OpenClaw автоматически обнаруживает только модели, которые сообщают о поддержке инструментов. Если ваша модель не указана, либо:
- Загрузите модель с поддержкой инструментов, или
- Определите модель явно в `models.providers.ollama`.

Чтобы добавить модели:

```bash
ollama list  # Посмотрите, что установлено
ollama pull llama3.3  # Загрузите модель
```

### Отказано в соединении

Проверьте, что Ollama работает на правильном порту:

```bash
# Проверьте, работает ли Ollama
ps aux | grep ollama

# Или перезапустите Ollama
ollama serve
```

## См. также

- [Провайдеры моделей](/concepts/model-providers) - Обзор всех провайдеров
- [Выбор модели](/concepts/models) - Как выбрать модели
- [Конфигурация](/gateway/configuration) - Полная справка по конфигурации
