---
summary: "OpenClaw가 Prompt 컨텍스트를 구축하고 Token 사용량 + 비용을 보고하는 방법"
read_when:
  - Token 사용량, 비용 또는 컨텍스트 창 설명
  - 컨텍스트 증가 또는 압축 동작 디버깅
---
# Token 사용 및 비용

OpenClaw는 문자가 아닌 **Token**을 추적합니다. Token은 모델별이지만 대부분의 OpenAI 스타일 모델은 영어 텍스트에 대해 Token당 평균 ~4자입니다.

## 시스템 Prompt가 구축되는 방법

OpenClaw는 실행할 때마다 자체 시스템 Prompt를 조립합니다. 포함 항목:

- Tool 목록 + 짧은 설명
- Skill 목록 (메타데이터만; 명령은 `read`로 요청 시 로드됨)
- 자체 업데이트 명령
- Workspace + 부트스트랩 파일 (`AGENTS.md`, `SOUL.md`, `TOOLS.md`, `IDENTITY.md`, `USER.md`, `HEARTBEAT.md`, 새로운 경우 `BOOTSTRAP.md`). 큰 파일은 `agents.defaults.bootstrapMaxChars` (기본값: 20000)에 의해 잘립니다.
- 시간 (UTC + 사용자 시간대)
- 답변 태그 + Heartbeat 동작
- 런타임 메타데이터 (호스트/OS/모델/Thinking)

전체 분석은 [System Prompt](/concepts/system-prompt)를 참조하세요.

## 컨텍스트 창에서 계산되는 것

모델이 받는 모든 것이 컨텍스트 제한에 포함됩니다:

- 시스템 Prompt (위에 나열된 모든 섹션)
- 대화 기록 (사용자 + Assistant 메시지)
- Tool 호출 및 Tool 결과
- 첨부 파일/전사 (이미지, 오디오, 파일)
- 압축 요약 및 프루닝 아티팩트
- Provider 래퍼 또는 안전 헤더 (보이지 않지만 여전히 계산됨)

주입된 파일, Tool, Skill 및 시스템 Prompt 크기별 실용적인 분석을 보려면 `/context list` 또는 `/context detail`을 사용하세요. [Context](/concepts/context)를 참조하세요.

## 현재 Token 사용량을 보는 방법

채팅에서 다음을 사용하세요:

- `/status` → Session 모델, 컨텍스트 사용량, 마지막 응답 입력/출력 Token 및 **예상 비용** (API 키만)이 포함된 **이모지가 풍부한 상태 카드**.
- `/usage off|tokens|full` → 모든 답변에 **응답별 사용량 바닥글**을 추가합니다.
  - Session별로 유지됩니다 (`responseUsage`로 저장됨).
  - OAuth 인증은 **비용을 숨깁니다** (Token만).
- `/usage cost` → OpenClaw Session 로그의 로컬 비용 요약을 표시합니다.

기타 표면:

- **TUI/Web TUI:** `/status` + `/usage`가 지원됩니다.
- **CLI:** `openclaw status --usage` 및 `openclaw channels list`는 Provider 할당량 창을 표시합니다 (응답별 비용 아님).

## 비용 추정 (표시될 때)

비용은 모델 가격 설정에서 추정됩니다:

```
models.providers.<provider>.models[].cost
```

이들은 `input`, `output`, `cacheRead` 및 `cacheWrite`에 대한 **1M Token당 USD**입니다. 가격이 누락된 경우 OpenClaw는 Token만 표시합니다. OAuth Token은 달러 비용을 표시하지 않습니다.

## 캐시 TTL 및 프루닝 영향

Provider Prompt 캐싱은 캐시 TTL 창 내에서만 적용됩니다. OpenClaw는 선택적으로 **캐시 TTL 프루닝**을 실행할 수 있습니다: 캐시 TTL이 만료되면 Session을 프루닝한 다음 캐시 창을 재설정하여 후속 요청이 전체 기록을 다시 캐싱하는 대신 새로 캐시된 컨텍스트를 재사용할 수 있도록 합니다. 이렇게 하면 Session이 TTL을 지나 유휴 상태일 때 캐시 쓰기 비용이 낮아집니다.

[Gateway configuration](/gateway/configuration)에서 구성하고 [Session pruning](/concepts/session-pruning)에서 동작 세부 정보를 참조하세요.

Heartbeat은 유휴 간격에서 캐시를 **따뜻하게** 유지할 수 있습니다. 모델 캐시 TTL이 `1h`인 경우 Heartbeat 간격을 그보다 약간 낮게 설정하면 (예: `55m`) 전체 Prompt를 다시 캐싱하는 것을 피할 수 있어 캐시 쓰기 비용이 감소합니다.

Anthropic API 가격의 경우 캐시 읽기는 입력 Token보다 훨씬 저렴하지만 캐시 쓰기는 더 높은 배수로 청구됩니다. 최신 요금 및 TTL 배수에 대해서는 Anthropic의 Prompt 캐싱 가격을 참조하세요:
https://docs.anthropic.com/docs/build-with-claude/prompt-caching

### 예시: Heartbeat으로 1시간 캐시 유지

```yaml
agents:
  defaults:
    model:
      primary: "anthropic/claude-opus-4-5"
    models:
      "anthropic/claude-opus-4-5":
        params:
          cacheControlTtl: "1h"
    heartbeat:
      every: "55m"
```

## Token 압력을 줄이기 위한 팁

- `/compact`를 사용하여 긴 Session을 요약하세요.
- 워크플로에서 큰 Tool 출력을 다듬으세요.
- Skill 설명을 짧게 유지하세요 (Skill 목록이 Prompt에 주입됨).
- 장황하고 탐색적인 작업에는 더 작은 모델을 선호하세요.

정확한 Skill 목록 오버헤드 공식은 [Skills](/tools/skills)를 참조하세요.
